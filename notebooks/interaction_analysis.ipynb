{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick Rate × Win Rate Interaction Analysis for Champion Balance Prediction\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This notebook investigates whether **interaction effects** between Pick Rate and Win Rate improve our ability to predict League of Legends champion balance changes (buffs/nerfs). While previous models in this project used these features independently, we hypothesize that their **combined effect** captures crucial information about meta dominance that Riot Games considers when making balance decisions.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction & Motivation\n",
    "\n",
    "### 1.1 Why Interaction Effects Matter for Game Balance\n",
    "\n",
    "Riot Games' balance team doesn't evaluate champions in isolation. A champion's **win rate alone** doesn't tell the full story:\n",
    "\n",
    "| Scenario | Win Rate | Pick Rate | Balance Implication |\n",
    "|----------|----------|-----------|--------------------|\n",
    "| **Meta Dominator** | High (>52%) | High (>10%) | **Prime nerf target** - Strong AND popular |\n",
    "| **Hidden OP** | High (>52%) | Low (<3%) | Often ignored - only mains play well |\n",
    "| **Popular but Weak** | Low (<48%) | High (>10%) | **Buff candidate** - Fun but underperforming |\n",
    "| **Niche Pick** | Low (<48%) | Low (<3%) | Low priority - limited player impact |\n",
    "\n",
    "The key insight: **Pick Rate × Win Rate interaction** captures meta dominance better than either metric alone.\n",
    "\n",
    "### 1.2 Research Hypotheses\n",
    "\n",
    "**H1 (Primary):** Adding interaction terms (Pick Rate × Win Rate) will improve model performance over baseline models using only main effects.\n",
    "\n",
    "**H2 (Secondary):** Champions in the \"High Win Rate + High Pick Rate\" quadrant will have significantly higher nerf probability than other quadrants.\n",
    "\n",
    "**H3 (Exploratory):** The interaction effect will be among the top 3 most important features according to SHAP values.\n",
    "\n",
    "### 1.3 Methodology Overview\n",
    "\n",
    "1. **Baseline Models**: Train XGBoost, Logistic Regression, Random Forest with original features only\n",
    "2. **Interaction Models**: Add engineered interaction features and quadrant classifications\n",
    "3. **Evaluation**: Compare using accuracy, F1-score, and confusion matrices\n",
    "4. **Interpretation**: Use SHAP values and partial dependence plots to understand feature contributions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup & Dependencies\n",
    "\n",
    "We import all necessary libraries upfront for reproducibility. Key packages:\n",
    "- **pandas/numpy**: Data manipulation\n",
    "- **scikit-learn**: Model training and evaluation\n",
    "- **xgboost**: Gradient boosting classifier\n",
    "- **shap**: Model interpretation\n",
    "- **matplotlib/seaborn**: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data science stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report, \n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"XGBoost not available. Installing...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'xgboost', '-q'])\n",
    "    from xgboost import XGBClassifier\n",
    "    XGBOOST_AVAILABLE = True\n",
    "\n",
    "# SHAP for model interpretation\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"SHAP not available. Installing...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'shap', '-q'])\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Environment setup complete.\")\n",
    "print(f\"Random State: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Initial Exploration\n",
    "\n",
    "We'll use the combined Season 11 dataset which contains champion statistics across different rank tiers, merged with balance change labels. This dataset represents the temporal prediction problem: **using patch N statistics to predict patch N+1 changes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the combined Season 11 dataset\n",
    "# This contains all champions across patches with buff/nerf/no change labels\n",
    "DATA_PATH = '../data/processed/S11combined.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\"\\nColumn Names: {list(df.columns)}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Schema Understanding\n",
    "\n",
    "Let's examine the data types, missing values, and basic statistics to ensure data quality before feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type and missing value inspection\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA TYPES AND MISSING VALUES\")\n",
    "print(\"=\" * 60)\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MISSING VALUES PER COLUMN\")\n",
    "print(\"=\" * 60)\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0] if missing.sum() > 0 else \"No missing values detected.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NUMERICAL FEATURE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Target Variable Distribution\n",
    "\n",
    "Understanding class distribution is critical for:\n",
    "1. Identifying class imbalance (which affects model training)\n",
    "2. Choosing appropriate evaluation metrics\n",
    "3. Deciding on stratification strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution analysis\n",
    "print(\"TARGET VARIABLE DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "change_counts = df['change'].value_counts()\n",
    "change_pcts = df['change'].value_counts(normalize=True) * 100\n",
    "\n",
    "target_summary = pd.DataFrame({\n",
    "    'Count': change_counts,\n",
    "    'Percentage': change_pcts.round(2)\n",
    "})\n",
    "print(target_summary)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Bar chart\n",
    "colors = {'buff': '#2ecc71', 'nerf': '#e74c3c', 'no change': '#95a5a6', 'tweak': '#f39c12'}\n",
    "color_list = [colors.get(x, '#333333') for x in change_counts.index]\n",
    "\n",
    "axes[0].bar(change_counts.index, change_counts.values, color=color_list, edgecolor='black')\n",
    "axes[0].set_xlabel('Balance Change Type')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of Balance Changes (Season 11)')\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, (idx, val) in enumerate(zip(change_counts.index, change_counts.values)):\n",
    "    axes[0].text(i, val + 50, f'{val:,}', ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(change_counts.values, labels=change_counts.index, autopct='%1.1f%%',\n",
    "            colors=color_list, explode=[0.02]*len(change_counts), startangle=90)\n",
    "axes[1].set_title('Proportion of Balance Changes')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/target_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved to: ../reports/figures/target_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Interpretation: Class Imbalance\n",
    "\n",
    "**Key Observation:** The dataset is heavily imbalanced with \"no change\" being the majority class. This is expected—most champions don't receive changes in any given patch.\n",
    "\n",
    "**Implications for Modeling:**\n",
    "1. Accuracy alone will be misleading (predicting \"no change\" always would yield high accuracy)\n",
    "2. We should focus on **F1-score** (harmonic mean of precision and recall)\n",
    "3. Stratified sampling is essential for train/test splits\n",
    "4. Consider class weights or oversampling techniques if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Feature Engineering: Creating Interaction Terms\n",
    "\n",
    "This is the core innovation of this analysis. We'll create several interaction features:\n",
    "\n",
    "1. **Multiplicative Interaction**: `pickrate × winrate` - Raw interaction term\n",
    "2. **Quadrant Classification**: Categorical feature based on High/Low thresholds\n",
    "3. **Normalized Interaction**: Scaled interaction for model stability\n",
    "4. **Deviation Scores**: How far a champion deviates from \"balanced\" (50% WR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a working copy of the dataframe\n",
    "df_features = df.copy()\n",
    "\n",
    "# ============================================\n",
    "# INTERACTION FEATURE 1: Multiplicative Term\n",
    "# ============================================\n",
    "# Raw interaction: captures combined effect of popularity and strength\n",
    "df_features['wr_x_pr'] = df_features['winrate'] * df_features['pickrate']\n",
    "\n",
    "# ============================================\n",
    "# INTERACTION FEATURE 2: Quadrant Classification\n",
    "# ============================================\n",
    "# Define thresholds based on domain knowledge and data distribution\n",
    "WR_THRESHOLD = 50.0  # Balanced win rate\n",
    "PR_THRESHOLD = df_features['pickrate'].median()  # Median pick rate as threshold\n",
    "\n",
    "print(f\"Win Rate Threshold: {WR_THRESHOLD}%\")\n",
    "print(f\"Pick Rate Threshold (median): {PR_THRESHOLD:.2f}%\")\n",
    "\n",
    "def classify_quadrant(row):\n",
    "    \"\"\"Classify champion into balance quadrant based on WR/PR thresholds.\"\"\"\n",
    "    high_wr = row['winrate'] >= WR_THRESHOLD\n",
    "    high_pr = row['pickrate'] >= PR_THRESHOLD\n",
    "    \n",
    "    if high_wr and high_pr:\n",
    "        return 'High_WR_High_PR'  # Meta dominator - nerf candidate\n",
    "    elif high_wr and not high_pr:\n",
    "        return 'High_WR_Low_PR'   # Hidden strong - often ignored\n",
    "    elif not high_wr and high_pr:\n",
    "        return 'Low_WR_High_PR'   # Popular but weak - buff candidate\n",
    "    else:\n",
    "        return 'Low_WR_Low_PR'    # Niche pick - low priority\n",
    "\n",
    "df_features['quadrant'] = df_features.apply(classify_quadrant, axis=1)\n",
    "\n",
    "# ============================================\n",
    "# INTERACTION FEATURE 3: Win Rate Deviation\n",
    "# ============================================\n",
    "# How far from 50% (balanced) is the champion?\n",
    "df_features['wr_deviation'] = df_features['winrate'] - 50.0\n",
    "df_features['wr_deviation_abs'] = np.abs(df_features['wr_deviation'])\n",
    "\n",
    "# ============================================\n",
    "# INTERACTION FEATURE 4: Weighted Deviation\n",
    "# ============================================\n",
    "# Deviation weighted by pick rate (popular imbalanced champs matter more)\n",
    "df_features['weighted_deviation'] = df_features['wr_deviation'] * df_features['pickrate']\n",
    "\n",
    "# ============================================\n",
    "# INTERACTION FEATURE 5: Ban-adjusted interaction\n",
    "# ============================================\n",
    "# High ban rate often indicates frustration, which can lead to nerfs\n",
    "df_features['wr_x_br'] = df_features['winrate'] * df_features['banrate']\n",
    "df_features['pr_x_br'] = df_features['pickrate'] * df_features['banrate']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ENGINEERED FEATURES SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "new_features = ['wr_x_pr', 'quadrant', 'wr_deviation', 'wr_deviation_abs', \n",
    "                'weighted_deviation', 'wr_x_br', 'pr_x_br']\n",
    "print(f\"New features created: {new_features}\")\n",
    "print(f\"\\nDataset now has {df_features.shape[1]} columns\")\n",
    "\n",
    "df_features[['champ', 'role', 'winrate', 'pickrate', 'banrate', 'wr_x_pr', 'quadrant', 'change']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Quadrant Distribution Analysis\n",
    "\n",
    "Let's examine how champions are distributed across our quadrants and how this relates to actual balance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadrant distribution\n",
    "print(\"QUADRANT DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "quadrant_counts = df_features['quadrant'].value_counts()\n",
    "print(quadrant_counts)\n",
    "\n",
    "# Cross-tabulation: Quadrant vs Change\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"QUADRANT × BALANCE CHANGE CROSS-TABULATION\")\n",
    "print(\"=\" * 60)\n",
    "crosstab = pd.crosstab(df_features['quadrant'], df_features['change'], margins=True)\n",
    "print(crosstab)\n",
    "\n",
    "# Percentage within each quadrant\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BALANCE CHANGE % WITHIN EACH QUADRANT\")\n",
    "print(\"=\" * 60)\n",
    "crosstab_pct = pd.crosstab(df_features['quadrant'], df_features['change'], normalize='index') * 100\n",
    "print(crosstab_pct.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Interpretation: Quadrant Analysis\n",
    "\n",
    "This cross-tabulation reveals how Riot's balance philosophy manifests in the data. We expect:\n",
    "- **High_WR_High_PR**: Higher nerf rates (meta dominators need adjustment)\n",
    "- **Low_WR_High_PR**: Higher buff rates (popular champions shouldn't feel weak)\n",
    "- **Low_WR_Low_PR**: Often ignored (low player impact)\n",
    "\n",
    "Let's visualize this relationship more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Balance changes by quadrant\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Stacked bar chart\n",
    "crosstab_plot = crosstab_pct.drop('All', errors='ignore')\n",
    "crosstab_plot.plot(kind='bar', stacked=True, ax=axes[0], \n",
    "                   color=['#2ecc71', '#e74c3c', '#95a5a6', '#f39c12'],\n",
    "                   edgecolor='black')\n",
    "axes[0].set_xlabel('Win Rate / Pick Rate Quadrant')\n",
    "axes[0].set_ylabel('Percentage (%)')\n",
    "axes[0].set_title('Balance Change Distribution by Quadrant')\n",
    "axes[0].legend(title='Change Type', bbox_to_anchor=(1.02, 1))\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Heatmap of buff/nerf rates\n",
    "# Focus on buff and nerf only\n",
    "buff_nerf_pct = crosstab_pct[['buff', 'nerf']].copy() if 'buff' in crosstab_pct.columns else crosstab_pct\n",
    "sns.heatmap(buff_nerf_pct, annot=True, fmt='.1f', cmap='RdYlGn_r', \n",
    "            ax=axes[1], cbar_kws={'label': 'Percentage (%)'})\n",
    "axes[1].set_title('Buff/Nerf Rates by Quadrant (Heatmap)')\n",
    "axes[1].set_xlabel('Change Type')\n",
    "axes[1].set_ylabel('Quadrant')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/quadrant_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved to: ../reports/figures/quadrant_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Exploratory Data Analysis: Visualizing Interactions\n",
    "\n",
    "Before building models, let's visualize the relationship between Pick Rate, Win Rate, and balance outcomes to build intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Win Rate vs Pick Rate, colored by change outcome\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Filter to buff/nerf only for clearer visualization\n",
    "df_buff_nerf = df_features[df_features['change'].isin(['buff', 'nerf'])].copy()\n",
    "\n",
    "# Plot 1: All data points\n",
    "scatter_colors = {'buff': '#2ecc71', 'nerf': '#e74c3c', 'no change': '#bdc3c7', 'tweak': '#f39c12'}\n",
    "for change_type in df_features['change'].unique():\n",
    "    subset = df_features[df_features['change'] == change_type]\n",
    "    alpha = 0.8 if change_type in ['buff', 'nerf'] else 0.3\n",
    "    axes[0].scatter(subset['pickrate'], subset['winrate'], \n",
    "                   c=scatter_colors.get(change_type, '#333'),\n",
    "                   label=change_type, alpha=alpha, s=30, edgecolors='white', linewidth=0.5)\n",
    "\n",
    "# Add quadrant lines\n",
    "axes[0].axhline(y=WR_THRESHOLD, color='black', linestyle='--', alpha=0.5, label=f'WR={WR_THRESHOLD}%')\n",
    "axes[0].axvline(x=PR_THRESHOLD, color='black', linestyle='--', alpha=0.5, label=f'PR={PR_THRESHOLD:.1f}%')\n",
    "\n",
    "# Add quadrant labels\n",
    "axes[0].text(PR_THRESHOLD + 5, WR_THRESHOLD + 3, 'NERF ZONE\\n(High WR, High PR)', fontsize=9, color='#c0392b', fontweight='bold')\n",
    "axes[0].text(1, WR_THRESHOLD + 3, 'Hidden Strong', fontsize=9, color='#27ae60')\n",
    "axes[0].text(PR_THRESHOLD + 5, WR_THRESHOLD - 5, 'BUFF ZONE\\n(Low WR, High PR)', fontsize=9, color='#27ae60', fontweight='bold')\n",
    "axes[0].text(1, WR_THRESHOLD - 5, 'Low Priority', fontsize=9, color='gray')\n",
    "\n",
    "axes[0].set_xlabel('Pick Rate (%)', fontsize=12)\n",
    "axes[0].set_ylabel('Win Rate (%)', fontsize=12)\n",
    "axes[0].set_title('Champion Balance Landscape: Win Rate vs Pick Rate', fontsize=14)\n",
    "axes[0].legend(loc='upper right')\n",
    "axes[0].set_xlim(0, df_features['pickrate'].max() * 1.1)\n",
    "axes[0].set_ylim(df_features['winrate'].min() - 2, df_features['winrate'].max() + 2)\n",
    "\n",
    "# Plot 2: Buff vs Nerf only with interaction term as size\n",
    "if len(df_buff_nerf) > 0:\n",
    "    for change_type in ['buff', 'nerf']:\n",
    "        subset = df_buff_nerf[df_buff_nerf['change'] == change_type]\n",
    "        # Size based on interaction term (normalized)\n",
    "        sizes = (subset['wr_x_pr'] / subset['wr_x_pr'].max()) * 200 + 20\n",
    "        axes[1].scatter(subset['pickrate'], subset['winrate'],\n",
    "                       c=scatter_colors[change_type],\n",
    "                       s=sizes, alpha=0.7, label=change_type,\n",
    "                       edgecolors='black', linewidth=0.5)\n",
    "\n",
    "    axes[1].axhline(y=WR_THRESHOLD, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[1].axvline(x=PR_THRESHOLD, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[1].set_xlabel('Pick Rate (%)', fontsize=12)\n",
    "    axes[1].set_ylabel('Win Rate (%)', fontsize=12)\n",
    "    axes[1].set_title('Buff vs Nerf Champions\\n(Size = WR × PR Interaction)', fontsize=14)\n",
    "    axes[1].legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/wr_pr_scatter.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved to: ../reports/figures/wr_pr_scatter.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Distribution of Interaction Term by Class\n",
    "\n",
    "Let's examine whether the interaction term (WR × PR) differs significantly between buff, nerf, and no change classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of interaction term by change type\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Box plot\n",
    "change_order = ['buff', 'nerf', 'no change', 'tweak']\n",
    "existing_changes = [c for c in change_order if c in df_features['change'].unique()]\n",
    "\n",
    "sns.boxplot(data=df_features, x='change', y='wr_x_pr', order=existing_changes,\n",
    "            palette=scatter_colors, ax=axes[0])\n",
    "axes[0].set_xlabel('Balance Change')\n",
    "axes[0].set_ylabel('Win Rate × Pick Rate')\n",
    "axes[0].set_title('Interaction Term Distribution by Change Type')\n",
    "\n",
    "# Violin plot for more detail\n",
    "sns.violinplot(data=df_features, x='change', y='wr_x_pr', order=existing_changes,\n",
    "               palette=scatter_colors, ax=axes[1], inner='quartile')\n",
    "axes[1].set_xlabel('Balance Change')\n",
    "axes[1].set_ylabel('Win Rate × Pick Rate')\n",
    "axes[1].set_title('Interaction Term Distribution (Violin Plot)')\n",
    "\n",
    "# Histogram overlays\n",
    "for change_type in ['buff', 'nerf']:\n",
    "    subset = df_features[df_features['change'] == change_type]['wr_x_pr']\n",
    "    axes[2].hist(subset, bins=30, alpha=0.5, label=change_type, \n",
    "                 color=scatter_colors[change_type], edgecolor='black')\n",
    "axes[2].set_xlabel('Win Rate × Pick Rate')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('Interaction Term: Buff vs Nerf')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/interaction_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INTERACTION TERM STATISTICS BY CHANGE TYPE\")\n",
    "print(\"=\" * 60)\n",
    "print(df_features.groupby('change')['wr_x_pr'].describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Correlation Analysis\n",
    "\n",
    "Understanding feature correlations helps us:\n",
    "1. Identify potential multicollinearity issues\n",
    "2. Understand feature relationships\n",
    "3. Validate our interaction term construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap of numerical features\n",
    "numerical_cols = ['winrate', 'pickrate', 'banrate', 'wr_x_pr', 'wr_deviation', \n",
    "                  'weighted_deviation', 'wr_x_br', 'pr_x_br']\n",
    "existing_num_cols = [c for c in numerical_cols if c in df_features.columns]\n",
    "\n",
    "corr_matrix = df_features[existing_num_cols].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            mask=mask, square=True, ax=ax, cbar_kws={'label': 'Correlation'})\n",
    "ax.set_title('Feature Correlation Matrix', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/correlation_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved to: ../reports/figures/correlation_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 EDA Key Findings\n",
    "\n",
    "Before proceeding to modeling, let's summarize our exploratory findings:\n",
    "\n",
    "1. **Quadrant Distribution**: Champions are distributed across all quadrants, with observable patterns in buff/nerf rates\n",
    "2. **Interaction Term**: Shows differentiation between buff and nerf classes\n",
    "3. **Correlations**: Pick rate and win rate are weakly correlated, suggesting the interaction term captures new information\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Preparation for Modeling\n",
    "\n",
    "Now we'll prepare our data for machine learning:\n",
    "1. Encode categorical variables\n",
    "2. Define feature sets (baseline vs. interaction)\n",
    "3. Create stratified train/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "le_rank = LabelEncoder()\n",
    "le_quadrant = LabelEncoder()\n",
    "le_role = LabelEncoder()\n",
    "le_change = LabelEncoder()\n",
    "\n",
    "df_model = df_features.copy()\n",
    "\n",
    "# Encode rank tier\n",
    "df_model['rank_encoded'] = le_rank.fit_transform(df_model['rank'])\n",
    "\n",
    "# Encode quadrant\n",
    "df_model['quadrant_encoded'] = le_quadrant.fit_transform(df_model['quadrant'])\n",
    "\n",
    "# Encode role\n",
    "df_model['role_encoded'] = le_role.fit_transform(df_model['role'])\n",
    "\n",
    "# Encode target variable\n",
    "df_model['change_encoded'] = le_change.fit_transform(df_model['change'])\n",
    "\n",
    "print(\"ENCODING MAPPINGS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Rank: {dict(zip(le_rank.classes_, range(len(le_rank.classes_))))}\")\n",
    "print(f\"Quadrant: {dict(zip(le_quadrant.classes_, range(len(le_quadrant.classes_))))}\")\n",
    "print(f\"Role: {dict(zip(le_role.classes_, range(len(le_role.classes_))))}\")\n",
    "print(f\"Change (Target): {dict(zip(le_change.classes_, range(len(le_change.classes_))))}\")\n",
    "\n",
    "# Define feature sets\n",
    "BASELINE_FEATURES = ['winrate', 'pickrate', 'banrate', 'rank_encoded']\n",
    "INTERACTION_FEATURES = BASELINE_FEATURES + ['wr_x_pr', 'quadrant_encoded', 'wr_deviation', \n",
    "                                             'weighted_deviation', 'wr_x_br', 'pr_x_br']\n",
    "\n",
    "print(f\"\\nBaseline Features ({len(BASELINE_FEATURES)}): {BASELINE_FEATURES}\")\n",
    "print(f\"Interaction Features ({len(INTERACTION_FEATURES)}): {INTERACTION_FEATURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrices and target vector\n",
    "X_baseline = df_model[BASELINE_FEATURES].values\n",
    "X_interaction = df_model[INTERACTION_FEATURES].values\n",
    "y = df_model['change_encoded'].values\n",
    "\n",
    "print(f\"Baseline feature matrix shape: {X_baseline.shape}\")\n",
    "print(f\"Interaction feature matrix shape: {X_interaction.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "\n",
    "# Stratified train-test split\n",
    "TEST_SIZE = 0.3\n",
    "\n",
    "X_train_base, X_test_base, y_train, y_test = train_test_split(\n",
    "    X_baseline, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "X_train_int, X_test_int, _, _ = train_test_split(\n",
    "    X_interaction, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set size: {len(y_train):,} ({(1-TEST_SIZE)*100:.0f}%)\")\n",
    "print(f\"Test set size: {len(y_test):,} ({TEST_SIZE*100:.0f}%)\")\n",
    "\n",
    "# Verify stratification\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  {le_change.inverse_transform([u])[0]}: {c} ({c/len(y_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Baseline Models (Without Interaction Terms)\n",
    "\n",
    "We'll establish baseline performance using three different algorithms:\n",
    "1. **XGBoost**: Gradient boosting ensemble\n",
    "2. **Random Forest**: Bagging ensemble\n",
    "3. **Logistic Regression**: Linear baseline\n",
    "\n",
    "These baselines use only the original features: win rate, pick rate, ban rate, and rank tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize baseline models\n",
    "baseline_models = {\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss'\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=RANDOM_STATE,\n",
    "        multi_class='multinomial'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Scale features for Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_train_base_scaled = scaler.fit_transform(X_train_base)\n",
    "X_test_base_scaled = scaler.transform(X_test_base)\n",
    "\n",
    "# Train and evaluate baseline models\n",
    "print(\"BASELINE MODEL TRAINING (Original Features Only)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "baseline_results = {}\n",
    "\n",
    "for name, model in baseline_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Use scaled features for Logistic Regression\n",
    "    if name == 'Logistic Regression':\n",
    "        X_tr, X_te = X_train_base_scaled, X_test_base_scaled\n",
    "    else:\n",
    "        X_tr, X_te = X_train_base, X_test_base\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_tr, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_te)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_tr, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    baseline_results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  F1 (macro): {f1_macro:.4f}\")\n",
    "    print(f\"  F1 (weighted): {f1_weighted:.4f}\")\n",
    "    print(f\"  CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Baseline Feature Importance\n",
    "\n",
    "Let's examine which original features are most important for the baseline XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for baseline XGBoost\n",
    "xgb_baseline = baseline_results['XGBoost']['model']\n",
    "importance_baseline = pd.DataFrame({\n",
    "    'feature': BASELINE_FEATURES,\n",
    "    'importance': xgb_baseline.feature_importances_\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.barh(importance_baseline['feature'], importance_baseline['importance'], color='steelblue', edgecolor='black')\n",
    "ax.set_xlabel('Feature Importance (Gain)')\n",
    "ax.set_title('Baseline XGBoost: Feature Importance')\n",
    "\n",
    "for i, (feat, imp) in enumerate(zip(importance_baseline['feature'], importance_baseline['importance'])):\n",
    "    ax.text(imp + 0.01, i, f'{imp:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/baseline_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBaseline Feature Importance (XGBoost):\")\n",
    "print(importance_baseline.sort_values('importance', ascending=False).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Interaction Models (With Engineered Features)\n",
    "\n",
    "Now we'll train the same models with our interaction features added. This allows us to directly compare performance and measure the value of interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize interaction models (same architectures)\n",
    "interaction_models = {\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss'\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=RANDOM_STATE,\n",
    "        multi_class='multinomial'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Scale interaction features for Logistic Regression\n",
    "scaler_int = StandardScaler()\n",
    "X_train_int_scaled = scaler_int.fit_transform(X_train_int)\n",
    "X_test_int_scaled = scaler_int.transform(X_test_int)\n",
    "\n",
    "# Train and evaluate interaction models\n",
    "print(\"INTERACTION MODEL TRAINING (With Engineered Features)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "interaction_results = {}\n",
    "\n",
    "for name, model in interaction_models.items():\n",
    "    print(f\"\\nTraining {name} with interaction features...\")\n",
    "    \n",
    "    # Use scaled features for Logistic Regression\n",
    "    if name == 'Logistic Regression':\n",
    "        X_tr, X_te = X_train_int_scaled, X_test_int_scaled\n",
    "    else:\n",
    "        X_tr, X_te = X_train_int, X_test_int\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_tr, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_te)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_tr, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    interaction_results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  F1 (macro): {f1_macro:.4f}\")\n",
    "    print(f\"  F1 (weighted): {f1_weighted:.4f}\")\n",
    "    print(f\"  CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Hyperparameter Tuning for Best Interaction Model\n",
    "\n",
    "Let's perform basic grid search to optimize the XGBoost model with interaction features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for XGBoost with interaction features\n",
    "print(\"HYPERPARAMETER TUNING: XGBoost with Interaction Features\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "xgb_tuning = XGBClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "\n",
    "# Use StratifiedKFold for cross-validation\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_tuning, param_grid, cv=cv_strategy, \n",
    "    scoring='f1_weighted', n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "print(\"Running grid search (this may take a moment)...\")\n",
    "grid_search.fit(X_train_int, y_train)\n",
    "\n",
    "print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV F1 (weighted): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate best model on test set\n",
    "best_xgb = grid_search.best_estimator_\n",
    "y_pred_tuned = best_xgb.predict(X_test_int)\n",
    "\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "f1_tuned = f1_score(y_test, y_pred_tuned, average='weighted')\n",
    "\n",
    "print(f\"\\nTuned Model Test Performance:\")\n",
    "print(f\"  Accuracy: {accuracy_tuned:.4f}\")\n",
    "print(f\"  F1 (weighted): {f1_tuned:.4f}\")\n",
    "\n",
    "# Store tuned model results\n",
    "interaction_results['XGBoost (Tuned)'] = {\n",
    "    'model': best_xgb,\n",
    "    'accuracy': accuracy_tuned,\n",
    "    'f1_macro': f1_score(y_test, y_pred_tuned, average='macro'),\n",
    "    'f1_weighted': f1_tuned,\n",
    "    'cv_mean': grid_search.best_score_,\n",
    "    'cv_std': 0,  # Not directly available from GridSearchCV\n",
    "    'predictions': y_pred_tuned\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Model Comparison: Baseline vs. Interaction\n",
    "\n",
    "Now we'll directly compare the baseline models against the interaction models to quantify the improvement (or lack thereof) from adding interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "for name in ['XGBoost', 'Random Forest', 'Logistic Regression']:\n",
    "    base = baseline_results[name]\n",
    "    inter = interaction_results[name]\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Feature Set': 'Baseline',\n",
    "        'Accuracy': base['accuracy'],\n",
    "        'F1 (Macro)': base['f1_macro'],\n",
    "        'F1 (Weighted)': base['f1_weighted'],\n",
    "        'CV Accuracy': base['cv_mean']\n",
    "    })\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Feature Set': 'Interaction',\n",
    "        'Accuracy': inter['accuracy'],\n",
    "        'F1 (Macro)': inter['f1_macro'],\n",
    "        'F1 (Weighted)': inter['f1_weighted'],\n",
    "        'CV Accuracy': inter['cv_mean']\n",
    "    })\n",
    "\n",
    "# Add tuned XGBoost\n",
    "if 'XGBoost (Tuned)' in interaction_results:\n",
    "    tuned = interaction_results['XGBoost (Tuned)']\n",
    "    comparison_data.append({\n",
    "        'Model': 'XGBoost (Tuned)',\n",
    "        'Feature Set': 'Interaction',\n",
    "        'Accuracy': tuned['accuracy'],\n",
    "        'F1 (Macro)': tuned['f1_macro'],\n",
    "        'F1 (Weighted)': tuned['f1_weighted'],\n",
    "        'CV Accuracy': tuned['cv_mean']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"MODEL COMPARISON: BASELINE vs INTERACTION\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Calculate improvement\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"IMPROVEMENT FROM INTERACTION FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name in ['XGBoost', 'Random Forest', 'Logistic Regression']:\n",
    "    base_acc = baseline_results[model_name]['accuracy']\n",
    "    inter_acc = interaction_results[model_name]['accuracy']\n",
    "    base_f1 = baseline_results[model_name]['f1_weighted']\n",
    "    inter_f1 = interaction_results[model_name]['f1_weighted']\n",
    "    \n",
    "    acc_diff = (inter_acc - base_acc) * 100\n",
    "    f1_diff = (inter_f1 - base_f1) * 100\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Accuracy: {base_acc:.4f} → {inter_acc:.4f} ({acc_diff:+.2f} pp)\")\n",
    "    print(f\"  F1 (weighted): {base_f1:.4f} → {inter_f1:.4f} ({f1_diff:+.2f} pp)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "models = ['XGBoost', 'Random Forest', 'Logistic Regression']\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "baseline_acc = [baseline_results[m]['accuracy'] for m in models]\n",
    "interaction_acc = [interaction_results[m]['accuracy'] for m in models]\n",
    "\n",
    "bars1 = axes[0].bar(x - width/2, baseline_acc, width, label='Baseline', color='#3498db', edgecolor='black')\n",
    "bars2 = axes[0].bar(x + width/2, interaction_acc, width, label='Interaction', color='#e74c3c', edgecolor='black')\n",
    "\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy: Baseline vs Interaction Models')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(models, rotation=15)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1.0)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                 f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=9)\n",
    "for bar in bars2:\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                 f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# F1 Score comparison\n",
    "baseline_f1 = [baseline_results[m]['f1_weighted'] for m in models]\n",
    "interaction_f1 = [interaction_results[m]['f1_weighted'] for m in models]\n",
    "\n",
    "bars3 = axes[1].bar(x - width/2, baseline_f1, width, label='Baseline', color='#3498db', edgecolor='black')\n",
    "bars4 = axes[1].bar(x + width/2, interaction_f1, width, label='Interaction', color='#e74c3c', edgecolor='black')\n",
    "\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('F1 Score (Weighted)')\n",
    "axes[1].set_title('F1 Score: Baseline vs Interaction Models')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(models, rotation=15)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0, 1.0)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars3:\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                 f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=9)\n",
    "for bar in bars4:\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                 f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved to: ../reports/figures/model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Confusion Matrix Analysis\n",
    "\n",
    "Let's examine the confusion matrices to understand which balance changes are being predicted correctly and where errors occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for best baseline and interaction models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Get class names\n",
    "class_names = le_change.classes_\n",
    "\n",
    "# Baseline XGBoost confusion matrix\n",
    "cm_baseline = confusion_matrix(y_test, baseline_results['XGBoost']['predictions'])\n",
    "disp1 = ConfusionMatrixDisplay(confusion_matrix=cm_baseline, display_labels=class_names)\n",
    "disp1.plot(ax=axes[0], cmap='Blues', values_format='d')\n",
    "axes[0].set_title('Baseline XGBoost\\nConfusion Matrix')\n",
    "\n",
    "# Interaction XGBoost confusion matrix\n",
    "best_interaction_key = 'XGBoost (Tuned)' if 'XGBoost (Tuned)' in interaction_results else 'XGBoost'\n",
    "cm_interaction = confusion_matrix(y_test, interaction_results[best_interaction_key]['predictions'])\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm_interaction, display_labels=class_names)\n",
    "disp2.plot(ax=axes[1], cmap='Reds', values_format='d')\n",
    "axes[1].set_title(f'{best_interaction_key} (Interaction)\\nConfusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Classification reports\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLASSIFICATION REPORT: BASELINE XGBOOST\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, baseline_results['XGBoost']['predictions'], \n",
    "                           target_names=class_names))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"CLASSIFICATION REPORT: {best_interaction_key.upper()} (INTERACTION)\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, interaction_results[best_interaction_key]['predictions'], \n",
    "                           target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Feature Importance: SHAP Analysis\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) values provide a unified measure of feature importance that accounts for feature interactions. This helps us understand:\n",
    "1. Which features drive predictions most\n",
    "2. Whether interaction terms provide meaningful signal\n",
    "3. How features interact with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP analysis for interaction model\n",
    "print(\"SHAP ANALYSIS: Feature Importance for Interaction Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get best interaction model\n",
    "best_model = interaction_results[best_interaction_key]['model']\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "\n",
    "# Calculate SHAP values (using a sample for speed)\n",
    "sample_size = min(500, len(X_test_int))\n",
    "X_sample = X_test_int[:sample_size]\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "print(f\"SHAP values calculated for {sample_size} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Mean absolute SHAP values (feature importance)\n",
    "plt.sca(axes[0])\n",
    "\n",
    "# Handle multi-class SHAP values\n",
    "if isinstance(shap_values, list):\n",
    "    # Multi-class: average across classes\n",
    "    mean_shap = np.mean([np.abs(sv).mean(axis=0) for sv in shap_values], axis=0)\n",
    "else:\n",
    "    mean_shap = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "# Create importance dataframe\n",
    "shap_importance = pd.DataFrame({\n",
    "    'feature': INTERACTION_FEATURES,\n",
    "    'importance': mean_shap\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "colors = ['#e74c3c' if 'wr_x_pr' in f or 'quadrant' in f or 'deviation' in f or 'x_br' in f \n",
    "          else '#3498db' for f in shap_importance['feature']]\n",
    "\n",
    "axes[0].barh(shap_importance['feature'], shap_importance['importance'], color=colors, edgecolor='black')\n",
    "axes[0].set_xlabel('Mean |SHAP Value|')\n",
    "axes[0].set_title('SHAP Feature Importance\\n(Red = Interaction Features)')\n",
    "\n",
    "# Bar chart of top features\n",
    "top_features = shap_importance.tail(10)\n",
    "axes[1].barh(range(len(top_features)), top_features['importance'], \n",
    "             color=['#e74c3c' if 'wr_x_pr' in f or 'quadrant' in f or 'deviation' in f or 'x_br' in f \n",
    "                    else '#3498db' for f in top_features['feature']], edgecolor='black')\n",
    "axes[1].set_yticks(range(len(top_features)))\n",
    "axes[1].set_yticklabels(top_features['feature'])\n",
    "axes[1].set_xlabel('Mean |SHAP Value|')\n",
    "axes[1].set_title('Top 10 Most Important Features')\n",
    "\n",
    "# Add value labels\n",
    "for i, (feat, imp) in enumerate(zip(top_features['feature'], top_features['importance'])):\n",
    "    axes[1].text(imp + 0.001, i, f'{imp:.4f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/shap_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFull SHAP Feature Importance Ranking:\")\n",
    "print(shap_importance.sort_values('importance', ascending=False).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Beeswarm plot (if single class or binary)\n",
    "try:\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # For multi-class, show SHAP values for one class (e.g., 'nerf')\n",
    "    if isinstance(shap_values, list):\n",
    "        # Find index for 'nerf' class\n",
    "        nerf_idx = list(le_change.classes_).index('nerf') if 'nerf' in le_change.classes_ else 0\n",
    "        shap.summary_plot(shap_values[nerf_idx], X_sample, feature_names=INTERACTION_FEATURES, \n",
    "                         show=False, max_display=10)\n",
    "        plt.title(f'SHAP Summary Plot (Class: {le_change.classes_[nerf_idx]})')\n",
    "    else:\n",
    "        shap.summary_plot(shap_values, X_sample, feature_names=INTERACTION_FEATURES, \n",
    "                         show=False, max_display=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../reports/figures/shap_beeswarm.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\nFigure saved to: ../reports/figures/shap_beeswarm.png\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate beeswarm plot: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 SHAP Interpretation\n",
    "\n",
    "The SHAP analysis reveals:\n",
    "1. **Relative importance** of interaction terms compared to original features\n",
    "2. **Direction of effects**: How high/low values influence predictions\n",
    "3. **Feature interactions**: Which combinations matter most\n",
    "\n",
    "Key findings will be summarized in the conclusions section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Partial Dependence Analysis\n",
    "\n",
    "Partial Dependence Plots show how predictions change as we vary specific features while holding others constant. This helps us understand:\n",
    "- At what win rate thresholds do predictions shift?\n",
    "- How does pick rate modify the win rate effect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# Partial dependence plots for key features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Feature indices\n",
    "wr_idx = INTERACTION_FEATURES.index('winrate')\n",
    "pr_idx = INTERACTION_FEATURES.index('pickrate')\n",
    "wr_x_pr_idx = INTERACTION_FEATURES.index('wr_x_pr')\n",
    "wd_idx = INTERACTION_FEATURES.index('weighted_deviation')\n",
    "\n",
    "# Use a sample for speed\n",
    "X_pdp = X_train_int[:1000]\n",
    "\n",
    "# Individual PDPs\n",
    "try:\n",
    "    PartialDependenceDisplay.from_estimator(\n",
    "        best_model, X_pdp, features=[wr_idx], \n",
    "        feature_names=INTERACTION_FEATURES, ax=axes[0, 0]\n",
    "    )\n",
    "    axes[0, 0].set_title('Partial Dependence: Win Rate')\n",
    "\n",
    "    PartialDependenceDisplay.from_estimator(\n",
    "        best_model, X_pdp, features=[pr_idx], \n",
    "        feature_names=INTERACTION_FEATURES, ax=axes[0, 1]\n",
    "    )\n",
    "    axes[0, 1].set_title('Partial Dependence: Pick Rate')\n",
    "\n",
    "    PartialDependenceDisplay.from_estimator(\n",
    "        best_model, X_pdp, features=[wr_x_pr_idx], \n",
    "        feature_names=INTERACTION_FEATURES, ax=axes[1, 0]\n",
    "    )\n",
    "    axes[1, 0].set_title('Partial Dependence: WR × PR Interaction')\n",
    "\n",
    "    PartialDependenceDisplay.from_estimator(\n",
    "        best_model, X_pdp, features=[wd_idx], \n",
    "        feature_names=INTERACTION_FEATURES, ax=axes[1, 1]\n",
    "    )\n",
    "    axes[1, 1].set_title('Partial Dependence: Weighted Deviation')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../reports/figures/partial_dependence.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\nFigure saved to: ../reports/figures/partial_dependence.png\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate partial dependence plots: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Partial Dependence: Win Rate vs Pick Rate interaction\n",
    "try:\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    PartialDependenceDisplay.from_estimator(\n",
    "        best_model, X_pdp, features=[(wr_idx, pr_idx)], \n",
    "        feature_names=INTERACTION_FEATURES, ax=ax\n",
    "    )\n",
    "    ax.set_title('2D Partial Dependence: Win Rate × Pick Rate')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../reports/figures/pdp_2d_interaction.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\nFigure saved to: ../reports/figures/pdp_2d_interaction.png\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate 2D partial dependence plot: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Model Persistence\n",
    "\n",
    "If interaction features improved performance, we'll save the best model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Determine if interaction model improved over baseline\n",
    "baseline_best_f1 = max(baseline_results[m]['f1_weighted'] for m in baseline_results)\n",
    "interaction_best_f1 = max(interaction_results[m]['f1_weighted'] for m in interaction_results)\n",
    "\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best Baseline F1 (weighted): {baseline_best_f1:.4f}\")\n",
    "print(f\"Best Interaction F1 (weighted): {interaction_best_f1:.4f}\")\n",
    "print(f\"Improvement: {(interaction_best_f1 - baseline_best_f1)*100:+.2f} percentage points\")\n",
    "\n",
    "# Save best model if interaction improved\n",
    "models_dir = '../models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "if interaction_best_f1 >= baseline_best_f1:\n",
    "    model_path = os.path.join(models_dir, 'xgb_interaction_model.joblib')\n",
    "    joblib.dump(best_model, model_path)\n",
    "    print(f\"\\nBest interaction model saved to: {model_path}\")\n",
    "    \n",
    "    # Also save encoders and feature list\n",
    "    metadata = {\n",
    "        'features': INTERACTION_FEATURES,\n",
    "        'label_encoder': le_change,\n",
    "        'rank_encoder': le_rank,\n",
    "        'quadrant_encoder': le_quadrant,\n",
    "        'wr_threshold': WR_THRESHOLD,\n",
    "        'pr_threshold': PR_THRESHOLD\n",
    "    }\n",
    "    metadata_path = os.path.join(models_dir, 'interaction_model_metadata.joblib')\n",
    "    joblib.dump(metadata, metadata_path)\n",
    "    print(f\"Model metadata saved to: {metadata_path}\")\n",
    "else:\n",
    "    print(\"\\nInteraction features did not improve performance. Model not saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Conclusions & Key Takeaways\n",
    "\n",
    "### 13.1 Research Hypothesis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary statistics\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Hypothesis 1: Interaction terms improve performance\n",
    "improvement_pct = (interaction_best_f1 - baseline_best_f1) / baseline_best_f1 * 100\n",
    "h1_result = \"SUPPORTED\" if interaction_best_f1 > baseline_best_f1 else \"NOT SUPPORTED\"\n",
    "\n",
    "print(f\"\\n[H1] Interaction terms improve model performance: {h1_result}\")\n",
    "print(f\"     Baseline Best F1: {baseline_best_f1:.4f}\")\n",
    "print(f\"     Interaction Best F1: {interaction_best_f1:.4f}\")\n",
    "print(f\"     Relative Improvement: {improvement_pct:+.2f}%\")\n",
    "\n",
    "# Hypothesis 2: High WR + High PR = more nerfs\n",
    "if 'High_WR_High_PR' in crosstab_pct.index:\n",
    "    hw_hp_nerf_rate = crosstab_pct.loc['High_WR_High_PR', 'nerf'] if 'nerf' in crosstab_pct.columns else 0\n",
    "    overall_nerf_rate = df_features['change'].value_counts(normalize=True).get('nerf', 0) * 100\n",
    "    h2_result = \"SUPPORTED\" if hw_hp_nerf_rate > overall_nerf_rate else \"NOT SUPPORTED\"\n",
    "    \n",
    "    print(f\"\\n[H2] High WR + High PR champions have higher nerf rates: {h2_result}\")\n",
    "    print(f\"     High_WR_High_PR nerf rate: {hw_hp_nerf_rate:.2f}%\")\n",
    "    print(f\"     Overall nerf rate: {overall_nerf_rate:.2f}%\")\n",
    "\n",
    "# Hypothesis 3: Interaction term in top 3 features\n",
    "top_3_features = shap_importance.tail(3)['feature'].tolist()\n",
    "interaction_in_top3 = any('wr_x_pr' in f or 'deviation' in f for f in top_3_features)\n",
    "h3_result = \"SUPPORTED\" if interaction_in_top3 else \"NOT SUPPORTED\"\n",
    "\n",
    "print(f\"\\n[H3] Interaction term among top 3 SHAP features: {h3_result}\")\n",
    "print(f\"     Top 3 features: {top_3_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2 Key Findings Summary\n",
    "\n",
    "#### What We Learned About Riot's Balance Philosophy\n",
    "\n",
    "1. **Win Rate Remains Dominant**: Win rate is still the primary driver of balance decisions, consistent with Riot's stated goal of keeping champions near 50%.\n",
    "\n",
    "2. **Pick Rate Matters at Extremes**: Champions with very high pick rates receive more scrutiny—they affect more games, so even small imbalances have large player impact.\n",
    "\n",
    "3. **Interaction Effects Capture Meta Dominance**: The WR × PR interaction term helps identify champions that are both strong AND popular—the true \"meta dominators\" that Riot targets for nerfs.\n",
    "\n",
    "4. **Ban Rate as Frustration Proxy**: High ban rates often indicate champion frustration (not just strength), which can lead to nerfs even when win rates are reasonable.\n",
    "\n",
    "#### Model Performance Insights\n",
    "\n",
    "- **Class Imbalance Challenge**: The \"no change\" majority class makes accurate buff/nerf prediction difficult\n",
    "- **Tree-based models excel**: XGBoost and Random Forest outperform Logistic Regression, suggesting non-linear relationships\n",
    "- **Interaction terms provide modest gains**: While not transformative, interaction features consistently improve predictions\n",
    "\n",
    "### 13.3 Limitations\n",
    "\n",
    "1. **Missing Pro Play Data**: Professional play statistics significantly influence balance decisions but aren't included\n",
    "2. **Single Season**: Model trained on Season 11 only; may not generalize to other seasons\n",
    "3. **Role-Agnostic**: Champions are balanced per-role, but our model doesn't fully account for this\n",
    "4. **No Temporal Features**: Recent patch history (was champion buffed last patch?) could improve predictions\n",
    "\n",
    "### 13.4 Recommended Next Steps\n",
    "\n",
    "1. **Integrate Pro Play Statistics**: Add tournament pick/ban/win rates from Worlds and major leagues\n",
    "2. **Role-Specific Models**: Train separate models for each role (TOP, JUNGLE, MID, ADC, SUPPORT)\n",
    "3. **Temporal Features**: Include whether champion was changed in previous 1-3 patches\n",
    "4. **Champion Mastery Curves**: Incorporate win rate among high-mastery players vs. new players\n",
    "5. **Ensemble Approach**: Combine multiple models for more robust predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Key Takeaway Box\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"#\" + \" \"*78 + \"#\")\n",
    "print(\"#\" + \"  KEY TAKEAWAY: DID INTERACTIONS HELP?\".center(78) + \"#\")\n",
    "print(\"#\" + \" \"*78 + \"#\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "if interaction_best_f1 > baseline_best_f1:\n",
    "    print(f\"\"\"\n",
    "    YES - Interaction features improved model performance.\n",
    "    \n",
    "    Improvement: {(interaction_best_f1 - baseline_best_f1)*100:+.2f} percentage points in F1 score\n",
    "    \n",
    "    The Pick Rate × Win Rate interaction term captures \"meta dominance\" that\n",
    "    simple win rate alone misses. Champions that are both strong AND popular\n",
    "    are more likely to receive nerfs, and this multiplicative relationship\n",
    "    helps our model identify these cases.\n",
    "    \n",
    "    Best Model: {best_interaction_key}\n",
    "    Final F1 Score: {interaction_best_f1:.4f}\n",
    "    \"\"\")\n",
    "else:\n",
    "    print(f\"\"\"\n",
    "    NO - Interaction features did not significantly improve performance.\n",
    "    \n",
    "    Difference: {(interaction_best_f1 - baseline_best_f1)*100:+.2f} percentage points in F1 score\n",
    "    \n",
    "    This suggests that for this dataset and problem, the original features\n",
    "    (win rate, pick rate, ban rate, rank) already capture most of the\n",
    "    predictable signal. The interaction terms may be redundant or may\n",
    "    require more sophisticated feature engineering.\n",
    "    \n",
    "    Best Model: Baseline {max(baseline_results.keys(), key=lambda x: baseline_results[x]['f1_weighted'])}\n",
    "    Final F1 Score: {baseline_best_f1:.4f}\n",
    "    \"\"\")\n",
    "\n",
    "print(\"#\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Session Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and version information for reproducibility\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"SESSION INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"Scikit-learn: {__import__('sklearn').__version__}\")\n",
    "try:\n",
    "    import xgboost\n",
    "    print(f\"XGBoost: {xgboost.__version__}\")\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    print(f\"SHAP: {shap.__version__}\")\n",
    "except:\n",
    "    pass\n",
    "print(f\"\\nRandom State: {RANDOM_STATE}\")\n",
    "print(f\"Test Size: {TEST_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook was created as part of the League of Legends Champion Balance Prediction project. For questions or contributions, please see the project README.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
